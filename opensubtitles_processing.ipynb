{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from os import path, walk\n",
    "\n",
    "from xml.sax.handler import ContentHandler\n",
    "from xml.sax import SAXException, make_parser\n",
    "\n",
    "\n",
    "class OpenSubtitlesHandler(ContentHandler):\n",
    "    def initialize(self):\n",
    "        self.sentences = []\n",
    "        self.within_word = False\n",
    "\n",
    "    def startDocument(self):\n",
    "        self.initialize()\n",
    "\n",
    "    def startElement(self, tag, attrs):\n",
    "        if tag == 's':\n",
    "            self.sentences.append([])\n",
    "        if tag == 'w':\n",
    "            self.within_word = True\n",
    "\n",
    "    def endElement(self, tag):\n",
    "        if tag == 'w':\n",
    "            self.within_word = False\n",
    "\n",
    "    def characters(self, content):\n",
    "        if self.within_word:\n",
    "            self.sentences[-1].append(content)\n",
    "\n",
    "\n",
    "def parse_corpus(text_root):\n",
    "    handler = OpenSubtitlesHandler()\n",
    "    parser = make_parser()\n",
    "    parser.setContentHandler(handler)\n",
    "\n",
    "    parsed_corpus = {}\n",
    "    for root, dirs, files in walk(text_root):\n",
    "        for filename in files:\n",
    "            if not filename.endswith('xml'):\n",
    "                continue\n",
    "            full_filename = path.join(root, filename)\n",
    "            parser.parse(full_filename)\n",
    "            parsed_corpus[full_filename] = handler.sentences\n",
    "    return parsed_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "def make_vocabulary(in_parsed_docs, limit=100000):\n",
    "    wordcount = defaultdict(lambda: 0)\n",
    "    for doc in in_parsed_docs:\n",
    "        for sentence in doc:\n",
    "            for word in sentence:\n",
    "                wordcount[word.lower() if word != 'I' else word] += 1\n",
    "    wordcount_sorted = sorted(wordcount.items(), key=itemgetter(1), reverse=True)\n",
    "    result = set(map(itemgetter(0), wordcount_sorted[:limit]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "UNK = '__UNK__'\n",
    "\n",
    "\n",
    "def preprocess_text(in_parsed_docs):\n",
    "    docs = in_parsed_docs.values()\n",
    "    vocabulary = make_vocabulary(docs)\n",
    "    filtered_get = lambda word: word if word in vocabulary else UNK\n",
    "    result = []\n",
    "    for content in docs:\n",
    "        processed_content = []\n",
    "        for sentence in content:\n",
    "            processed_sentence = [word.lower() if word != 'I' else word for word in sentence]\n",
    "            filtered_sentence = [filtered_get(word) for word in processed_sentence]\n",
    "            processed_content.append(filtered_sentence)\n",
    "        result.append(processed_content)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "from os import path, makedirs\n",
    "\n",
    "TESTSET_SIZE_RATIO = 0.2\n",
    "\n",
    "\n",
    "def prepare_seq2seq_files(in_processed_docs, in_result_path):\n",
    "    if not path.exists(in_result_path):\n",
    "        makedirs(in_result_path)\n",
    "\n",
    "    qa_data = []\n",
    "    for doc in in_processed_docs:\n",
    "        for question, answer in zip(doc[::2], doc[1::2]):\n",
    "            qa_data.append((question, answer))\n",
    "\n",
    "    shuffle(qa_data)\n",
    "    \n",
    "    trainset_size = int((1 - TESTSET_SIZE_RATIO) * len(qa_data))\n",
    "    qa_train, qa_test = qa_data[:trainset_size], qa_data[trainset_size:]\n",
    "\n",
    "    # open files\n",
    "    with \\\n",
    "        open(path.join(in_result_path, 'train.enc'), 'w') as train_enc, \\\n",
    "        open(path.join(in_result_path, 'train.dec'), 'w') as train_dec, \\\n",
    "        open(path.join(in_result_path, 'test.enc'), 'w') as test_enc, \\\n",
    "        open(path.join(in_result_path, 'test.dec'), 'w') as test_dec:\n",
    "\n",
    "        for question_train, answer_train in qa_train:\n",
    "            print >>train_enc, ' '.join(question_train).encode('utf-8')\n",
    "            print >>train_dec, ' '.join(answer_train).encode('utf-8')\n",
    "        for question_test, answer_test in qa_test:\n",
    "            print >>test_enc, ' '.join(question_test).encode('utf-8')\n",
    "            print >>test_dec, ' '.join(answer_test).encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ORIGINAL_CORPUS_ROOT = 'OpenSubtitles'\n",
    "parsed_texts = parse_corpus(ORIGINAL_CORPUS_ROOT)\n",
    "processed_texts = preprocess_text(parsed_texts)\n",
    "prepare_seq2seq_files(processed_texts, 'opensubtitles_seq2seq_dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corpus Info\n",
    "=="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents number:\t2317\n",
      "Sentences number:\t2739528\n",
      "Words number:\t19922136\n"
     ]
    }
   ],
   "source": [
    "sentences_number = 0\n",
    "words_number = 0\n",
    "for doc in processed_texts:\n",
    "    sentences_number += len(doc)\n",
    "    for sentence in doc:\n",
    "        words_number += len(sentence)\n",
    "\n",
    "print 'Documents number:\\t{}'.format(len(processed_texts))\n",
    "print 'Sentences number:\\t{}'.format(sentences_number)\n",
    "print 'Words number:\\t{}'.format(words_number)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
